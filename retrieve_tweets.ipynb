{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9430dbc7-4398-48bb-a3cf-d8fd8478c5a2",
   "metadata": {},
   "source": [
    "# Set Tweeter Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13994c0-5954-49fe-b3e4-8daa01d35dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "API_KEY=\" \"\n",
    "API_KEY_SECRET=\" \"\n",
    "ACCESS_TOKEN=\" \"\n",
    "ACCESS_TOKEN_SECRET=\" \"\n",
    "BEARER_TOKEN=\" \"\n",
    "\n",
    "consumer_key = API_KEY\n",
    "consumer_secret = API_KEY_SECRET\n",
    "access_token = ACCESS_TOKEN\n",
    "access_token_secret = ACCESS_TOKEN_SECRET\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "  consumer_key, \n",
    "  consumer_secret, \n",
    "  access_token, \n",
    "  access_token_secret\n",
    ")\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print(api.verify_credentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40c702-0781-4777-89f8-093de2191e59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract the tweeets from Tweeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b9bf8-df9a-4f60-90fc-6d0ad1a28c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "bearer_token = \" \"\n",
    "\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "rules = [\n",
    "    {\"value\": '((\"chatGPT\" OR \"#GPT-4\" OR \"GPT4\" OR \"#chatgpt\")) -is:retweet lang:en', \"tag\": \"advice_18\"}\n",
    "]\n",
    "\n",
    "query_parameters = {\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at,public_metrics\",\n",
    "    \"user.fields\": \"id,name,username,created_at,description,location,verified\",\n",
    "    \"expansions\": \"author_id\",\n",
    "    \"max_results\": 100,\n",
    "}\n",
    "\n",
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sets up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "headers = request_headers(bearer_token)\n",
    "\n",
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def process_twitter_data(\n",
    "    json_response: json,\n",
    "    query_tag: str,\n",
    "    tweets_data: pd.DataFrame,\n",
    "    users_data: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds new tweet/user information to the table of\n",
    "    tweets/users and saves dataframes as pickle files,\n",
    "    if data is avaiable.\n",
    "    \n",
    "    Returns the tweets and users updated dataframes.\n",
    "    \"\"\"\n",
    "    if \"data\" in json_response.keys():\n",
    "        new = pd.DataFrame(json_response[\"data\"])\n",
    "        tweets_data = pd.concat([tweets_data, new])\n",
    "        tweets_data.to_pickle(\"./raw_tweets/tweets_\" + query_tag + \".pkl\")\n",
    "\n",
    "        if \"users\" in json_response[\"includes\"].keys():\n",
    "            new = pd.DataFrame(json_response[\"includes\"][\"users\"])\n",
    "            users_data = pd.concat([users_data, new])\n",
    "            users_data.drop_duplicates(\"id\", inplace=True)\n",
    "            users_data.to_pickle(\"./raw_tweets/users_\" + query_tag + \".pkl\")\n",
    "\n",
    "    return tweets_data, users_data\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "tweets_data = pd.DataFrame()\n",
    "users_data = pd.DataFrame()\n",
    "\n",
    "for i in range(len(rules)):\n",
    "    print(\"Working on rule \", i)\n",
    "    query_parameters[\"query\"] = rules[i][\"value\"]\n",
    "    query_tag = rules[i][\"tag\"]\n",
    "\n",
    "    json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "    tweets_data, users_data = process_twitter_data(\n",
    "       json_response, query_tag, tweets_data, users_data\n",
    "    )\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    while \"next_token\" in json_response[\"meta\"]:\n",
    "        query_parameters[\"next_token\"] = json_response[\"meta\"][\"next_token\"]\n",
    "\n",
    "        json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "        tweets_data, users_data = process_twitter_data(\n",
    "            json_response, query_tag, tweets_data, users_data\n",
    "        )\n",
    "\n",
    "        time.sleep(5)\n",
    "        \n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377259e1-1597-4f2a-9ae6-913e040636b2",
   "metadata": {},
   "source": [
    "# Concatanate all the extracted tweet files, and remove duplicates based on tweet ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42805e5-9e25-4235-89d0-91d59c190b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "files = glob.glob(\"./raw_tweets/tweets_*.pkl\")\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_pickle(file) \n",
    "    new_df = pd.concat([df.drop(['public_metrics'], axis=1), df['public_metrics'].apply(pd.Series)], axis=1)\n",
    "    new_df[\"rule\"] = str(file)\n",
    "    \n",
    "    full_df = pd.concat([full_df, new_df], axis=0)\n",
    "\n",
    "print(full_df.shape[0])\n",
    "full_df = full_df.drop_duplicates(subset=[\"id\"])\n",
    "print(full_df.shape[0])\n",
    "\n",
    "full_df.to_csv(\"./final_tweets_04062023.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
